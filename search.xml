<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Hello World</title>
    <url>/post/4a17b156.html</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<a id="more"></a>

<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>【译】用Java构建简单的神经网络</title>
    <url>/post/d4ab2505.html</url>
    <content><![CDATA[<blockquote>
<p>本篇文章翻译自一篇英文博客，加入了自己的一些理解，力求通俗易懂，向非专业人士科普神经网络，如有不足之处欢迎在评论区提出宝贵意见，阅读原文请戳<a href="https://smalldata.tech/blog/2016/05/03/building-a-simple-neural-net-in-java">原文链接</a></p>
</blockquote>
<p>在这篇文章中我们将使用简单的步骤来解决人工智能问题，并且尝试使用Java来构建一个简单的神经网络。</p>
<a id="more"></a>

<h3 id="什么是神经网络？"><a href="#什么是神经网络？" class="headerlink" title="什么是神经网络？"></a>什么是神经网络？</h3><p>神经网络是一种模拟大脑如何工作的数学模型。遗憾的是，我们还不知道大脑到底是如何工作的，但是我们确实知道一些背后的生物学原理：人类的大脑包含大约一千亿个神经元，它们被突触连接在一起，如果连接到神经元的突触被激活，那么神经元也会被激活。这个过程被称为“思考”。</p>
<p>因此我们尝试使用一个简单的案例来对上述过程进行建模，这个案例包含三个输入和一个输出。三个输入通过三个突触连接到一个神经元上，神经元激活后产生一个输出。</p>
<img src="https://cdn.jsdelivr.net/gh/Caleb411/image@main/20201223/nn-1-simple-problem.4xomqb0jbrg0.png" alt="nn-1-simple-problem" style="zoom:50%;" />

<h3 id="一个简单的问题"><a href="#一个简单的问题" class="headerlink" title="一个简单的问题"></a>一个简单的问题</h3><p>我们将训练上述神经网络来解决以下问题。你能找出其中隐含的规律并且猜出新的输入对应的输出是什么吗？0还是1？</p>
<table>
<thead>
<tr>
<th align="center">数据</th>
<th align="center">输入1</th>
<th align="center">输入2</th>
<th align="center">输入3</th>
<th align="center">输出</th>
</tr>
</thead>
<tbody><tr>
<td align="center">数据1</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">1</td>
<td align="center">0</td>
</tr>
<tr>
<td align="center">数据2</td>
<td align="center">1</td>
<td align="center">1</td>
<td align="center">1</td>
<td align="center">1</td>
</tr>
<tr>
<td align="center">数据3</td>
<td align="center">1</td>
<td align="center">0</td>
<td align="center">1</td>
<td align="center">1</td>
</tr>
<tr>
<td align="center">数据4</td>
<td align="center">0</td>
<td align="center">1</td>
<td align="center">1</td>
<td align="center">0</td>
</tr>
<tr>
<td align="center">新数据</td>
<td align="center">1</td>
<td align="center">1</td>
<td align="center">0</td>
<td align="center">?</td>
</tr>
</tbody></table>
<p>答案实际上就是最左边一列的值，即1！</p>
<h3 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h3><p>所以现在我们有一个人脑模型，我们将尝试让我们的神经网络去学习给定训练集（即所有用于学习的数据，每条数据包含三个输入和一个输出）中的规律。我们将首先赋予每个输入一个随机数字（即每个输入对应的权重）来产生一个输出。</p>
<img src="https://cdn.jsdelivr.net/gh/Caleb411/image@main/20201223/nn-1-simple-problem-neuron-weights.2zhuj6hzfn60.png" alt="nn-1-simple-problem-neuron-weights" style="zoom:50%;" />

<p>计算输出的公式如下：</p>
<p>$$\sum weight_i . input_i = weight1 . input1 + weight2 . input2 + weight3 . input3$$</p>
<p>现实中我们希望将输出值转化为0到1之间的某个数，使预测结果有意义，这个过程被称为“归一化”。我们将归一化后的输出与我们期望的输出（即训练集中的输出）作对比，这将产生错误，或者称之为与期望值之间的差距（即误差）。然后，我们可以根据错误对神经网络的权重进行微调，并且再次在同样的数据上进行尝试。这个过程可以总结成下面这张图：</p>
<img src="https://cdn.jsdelivr.net/gh/Caleb411/image@main/20201223/nn-1-training-process.6uz0pye9c340.png" alt="nn-1-training-process" style="zoom:50%;" />

<p>我们对所有数据（所有数据在每轮中计算一次误差）重复这样的训练过程<strong>10,000次</strong>，让神经网络获得充分训练。接下来我们可以使用这个神经网络对新的输入进行预测。</p>
<p>但是，在进行具体实现之前，我们仍然需要弄清楚归一化的过程是怎样的，以及如何根据错误来调整每个输入权重（也称之为反向传播）。</p>
<h3 id="归一化"><a href="#归一化" class="headerlink" title="归一化"></a>归一化</h3><p>在受生物学启发的神经网络中，一个神经元的输出通常代表着细胞中动作电位的触发。其最简单的形式为一个二进制数，即神经元是否被触发。因此，需要对输出值进行归一化。</p>
<p>为了实现这种归一化，我们将一个函数应用于神经元的输出，该函数被称为“激活函数”。如果我们把简单的跃阶函数（即将负数转化为0，正数转化为1）作为激活函数，则需要大量神经元来达到缓慢调整权重减少误差的目的，原因在于该函数输出非0即1，粒度太粗。</p>
<p>我们将在下一部分有关反向传播的内容中讲到，“缓慢调整权重”这一概念与数学语言中激活函数的斜率有关。从生物学的角度来说就是随着输入电流的增加，激活的速度就越快。如果我们使用线性函数而不是跃阶函数，那么我们会发现，由于线性函数的输出并不在0和1之间，因此在缓慢调整权重的过程中神经元的输出将趋于无限制地增加，因此最终的网络将具有不稳定的收敛性。</p>
<p>上面提到的所有问题都可以通过使用可归一化的S型激活函数来解决。可以类比现实中这样一个近似的模型，变量一开始保持为零，一旦接收到电流，开始迅速增加，并逐渐接近100％。从数学上讲，对应这样一个式子：</p>
<p>$$\frac{1}{1 + e^{-x}}$$</p>
<p>如果在图表上绘制，可以得到一条S型曲线：</p>
<img src="https://cdn.jsdelivr.net/gh/Caleb411/image@main/20201223/nn-1-sigmoid-plot.35xfodc4t4g0.png" alt="nn-1-sigmoid-plot" style="zoom:50%;" />

<p>因此，神经元输出的最终公式变为</p>
<p>$$Output = \frac{1}{1 + e^{-(\sum weight_i . input_i)}}$$</p>
<p>我们还可以使用其他归一化函数，但S型曲线的优点是非常简单，并且具有简单的导数，这将非常有利于我们接下来要讲的反向传播。</p>
<h3 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h3><p>在每轮的训练中，我们根据错误调整权重。为了做到这一点，我们引入“调整公式”</p>
<p>$$Adjustment = error . input . SigmoidCurveGradient(output)$$</p>
<p>我们使用这个公式的首要原因是我们需要根据误差的大小来调整权重。其次，我们将调整后的权重与输入（要么是0要么是1）相乘，所以调整与输入也有关，如果输入是0，则权重等于没有调整。最后，我们还要乘上S型曲线的梯度（或者称之为导数）。</p>
<p>为什么与梯度有关呢？因为我们要将错误最小化，即让神经网络尽可能少犯错。具体来说，可以通过<a href="https://en.wikipedia.org/wiki/Gradient_descent">梯度下降方法</a>来做到这一点。即从参数空间中的当前点开始（当前点由当前所有参数的权重确定），每次都朝着错误减小的方向调整。形象化的描述就是，站在山坡上并沿着坡度最陡的方向走。应用于神经网络的梯度下降方法如下所示：</p>
<ol>
<li><p>如果神经元的输出是一个较大的正数或负数，则表明该神经元非常倾向于认为结果是1或者是0。</p>
</li>
<li><p>从S型曲线图中可以看出，此时曲线的斜率较小。</p>
</li>
<li><p>因此，如果神经元确信当前的权重是正确的，那么它就不需要调整太多，而乘以S型曲线的斜率就可以实现这一点。</p>
</li>
</ol>
<p>S型函数的导数由下面的公式给出</p>
<p>$$SigmoidCurveGradient(output) = output . (1 - output)$$</p>
<p>将其代入调整公式可得到</p>
<p>$$Adjustment = error . input . output . (1 - output)$$</p>
<h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><p>在解释以上数学概念时，遗漏了重要但微妙的一点，对于每次训练迭代，数学运算都在整个训练集上同时完成。因此，我们将利用矩阵来分别存储输入向量，权重和预期输出的集合。</p>
<p>你可以在此处获取整个项目的源代码：<a href="https://github.com/wheresvic/neuralnet">https://github.com/wheresvic/neuralnet</a> 。为了方便学习，我们仅使用标准的Java Math函数自己实现了所有数学运算：)</p>
<p>我们将从<code>NeuronLayer</code>类开始，该类只是神经网络实现中的权重部分。我们为它提供每个神经元的输入数量以及可用于构建权重表的神经元数量。在我们当前的示例中，这只是具有3个输入神经元的最后一个输出神经元。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">NeuronLayer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">final</span> Function activationFunction, activationFunctionDerivative;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">double</span>[][] weights;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">NeuronLayer</span><span class="params">(<span class="keyword">int</span> numberOfNeurons, <span class="keyword">int</span> numberOfInputsPerNeuron)</span> </span>&#123;</span><br><span class="line">        weights = <span class="keyword">new</span> <span class="keyword">double</span>[numberOfInputsPerNeuron][numberOfNeurons];</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; numberOfInputsPerNeuron; ++i) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; numberOfNeurons; ++j) &#123;</span><br><span class="line">                weights[i][j] = (<span class="number">2</span> * Math.random()) - <span class="number">1</span>; <span class="comment">// 把范围从O~1调整为-1~1</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        activationFunction = NNMath::sigmoid;</span><br><span class="line">        activationFunctionDerivative = NNMath::sigmoidDerivative;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">adjustWeights</span><span class="params">(<span class="keyword">double</span>[][] adjustment)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.weights = NNMath.matrixAdd(weights, adjustment);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>我们的神经网络类是所有动作发生的地方。它以<code>NeuronLayer</code>作为构造函数，并具有2个主要功能：</p>
<ul>
<li><p><code>think</code>：计算给定输入集的输出</p>
</li>
<li><p><code>train</code>：运行训练循环<code>numberOfTrainingIterations</code>次（通常是10,000这个级别的数字）。请注意，训练本身涉及计算输出，然后相应地调整权重</p>
</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">NeuralNetSimple</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> NeuronLayer layer1;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">double</span>[][] outputLayer1;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">NeuralNetSimple</span><span class="params">(NeuronLayer layer1)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.layer1 = layer1;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">think</span><span class="params">(<span class="keyword">double</span>[][] inputs)</span> </span>&#123;</span><br><span class="line">        outputLayer1 = apply(matrixMultiply(inputs, layer1.weights), layer1.activationFunction);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">train</span><span class="params">(<span class="keyword">double</span>[][] inputs, <span class="keyword">double</span>[][] outputs, <span class="keyword">int</span> numberOfTrainingIterations)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; numberOfTrainingIterations; ++i) &#123;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 将训练集中的数据通过网络进行传播</span></span><br><span class="line">            think(inputs);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 通过 error * input * output * (1 - output) 调整权重</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">double</span>[][] errorLayer1 = matrixSubtract(outputs, outputLayer1);</span><br><span class="line">            <span class="keyword">double</span>[][] deltaLayer1 = scalarMultiply(errorLayer1, apply(outputLayer1, layer1.activationFunctionDerivative));</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 计算要调整多少</span></span><br><span class="line">            <span class="keyword">double</span>[][] adjustmentLayer1 = matrixMultiply(matrixTranspose(inputs), deltaLayer1);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 调整权重</span></span><br><span class="line">            <span class="keyword">this</span>.layer1.adjustWeights(adjustmentLayer1);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">double</span>[][] getOutput() &#123;</span><br><span class="line">        <span class="keyword">return</span> outputLayer1;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>最终，我们有了<code>main</code>方法来设置训练数据，训练网络并要求其对测试数据进行预测</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LearnFirstColumnSimple</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String args[])</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建包含一个神经元和三个输入的隐含层（神经网络中除去输入层和输出层，其他层被称为隐含层）</span></span><br><span class="line">        NeuronLayer layer1 = <span class="keyword">new</span> NeuronLayer(<span class="number">1</span>, <span class="number">3</span>);</span><br><span class="line"></span><br><span class="line">        NeuralNetSimple net = <span class="keyword">new</span> NeuralNetSimple(layer1);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 训练网络</span></span><br><span class="line">        <span class="keyword">double</span>[][] inputs = <span class="keyword">new</span> <span class="keyword">double</span>[][]&#123;</span><br><span class="line">                &#123;<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>&#125;,</span><br><span class="line">                &#123;<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>&#125;,</span><br><span class="line">                &#123;<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>&#125;,</span><br><span class="line">                &#123;<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>&#125;</span><br><span class="line">        &#125;;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">double</span>[][] outputs = <span class="keyword">new</span> <span class="keyword">double</span>[][]&#123;</span><br><span class="line">                &#123;<span class="number">0</span>&#125;,</span><br><span class="line">                &#123;<span class="number">1</span>&#125;,</span><br><span class="line">                &#123;<span class="number">1</span>&#125;,</span><br><span class="line">                &#123;<span class="number">0</span>&#125;</span><br><span class="line">        &#125;;</span><br><span class="line"></span><br><span class="line">        System.out.println(<span class="string">&quot;Training the neural net...&quot;</span>);</span><br><span class="line">        net.train(inputs, outputs, <span class="number">10000</span>);</span><br><span class="line">        System.out.println(<span class="string">&quot;Finished training&quot;</span>);</span><br><span class="line"></span><br><span class="line">        System.out.println(<span class="string">&quot;Layer 1 weights&quot;</span>);</span><br><span class="line">        System.out.println(layer1);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 计算对新数据的预测结果</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1, 0, 0</span></span><br><span class="line">        predict(<span class="keyword">new</span> <span class="keyword">double</span>[][], net);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 0, 1, 0</span></span><br><span class="line">        predict(<span class="keyword">new</span> <span class="keyword">double</span>[][], net);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1, 1, 0</span></span><br><span class="line">        predict(<span class="keyword">new</span> <span class="keyword">double</span>[][], net);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">predict</span><span class="params">(<span class="keyword">double</span>[][] testInput, NeuralNetSimple net)</span> </span>&#123;</span><br><span class="line">        net.think(testInput);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 打印结果</span></span><br><span class="line">        System.out.println(<span class="string">&quot;Prediction on data &quot;</span></span><br><span class="line">                + testInput[<span class="number">0</span>][<span class="number">0</span>] + <span class="string">&quot; &quot;</span></span><br><span class="line">                + testInput[<span class="number">0</span>][<span class="number">1</span>] + <span class="string">&quot; &quot;</span></span><br><span class="line">                + testInput[<span class="number">0</span>][<span class="number">2</span>] + <span class="string">&quot; -&gt; &quot;</span></span><br><span class="line">                + net.getOutput()[<span class="number">0</span>][<span class="number">0</span>] + <span class="string">&quot;, expected -&gt; &quot;</span> + testInput[<span class="number">0</span>][<span class="number">0</span>]);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>运行上面的示例，我们看到网络在预测最左边的输入为1的时候做得很好，但是为0的时候得到正确结果的概率似乎较低！这是因为训练结果中第二和第三输入权重都需要都接近0。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Training the neural net...</span><br><span class="line">Finished training</span><br><span class="line">Layer <span class="number">1</span> weights</span><br><span class="line">[[<span class="number">9.672988220005456</span> ]</span><br><span class="line">[-<span class="number">0.2089781536334558</span> ]</span><br><span class="line">[-<span class="number">4.628957430141331</span> ]</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">Prediction on data <span class="number">1.0</span> <span class="number">0.0</span> <span class="number">0.0</span> -&gt; <span class="number">0.9999370425325528</span>, expected -&gt; <span class="number">1.0</span></span><br><span class="line">Prediction on data <span class="number">0.0</span> <span class="number">1.0</span> <span class="number">0.0</span> -&gt; <span class="number">0.4479447696095623</span>, expected -&gt; <span class="number">0.0</span></span><br><span class="line">Prediction on data <span class="number">1.0</span> <span class="number">1.0</span> <span class="number">0.0</span> -&gt; <span class="number">0.9999224112145153</span>, expected -&gt; <span class="number">1.0</span></span><br></pre></td></tr></table></figure>

<p>在下一篇文章中，我们将看到在神经网络中额外添加一层是否有助于改善预测；）</p>
<h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><ul>
<li><a href="https://smalldata.tech/api/to/71f50e36800f1c71de359f15d9e84966">Milo Spencer-Harper</a>的教程，介绍如何使用python创建简单的神经网络。</li>
<li><a href="https://smalldata.tech/api/to/4a20b748ed0f184cd7c792509e721fbc">Steven Miller</a>撰写的有关构建简单神经网络的教程。</li>
<li>维基百科有关<a href="https://en.wikipedia.org/wiki/Activation_function">激活函数</a>的文章</li>
</ul>
]]></content>
      <categories>
        <category>文章翻译</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>个人博客搭建教程</title>
    <url>/post/26a0c682.html</url>
    <content><![CDATA[<p>这篇文章详细记录了基于Hexo+Github搭建个人博客的简要过程，帮助你免费搭建个人博客。</p>
<a id="more"></a>

<p>下载安装node.js <a href="https://nodejs.org/en/">https://nodejs.org/en/</a></p>
<p>下载安装git <a href="https://git-scm.com/downloads">https://git-scm.com/downloads</a></p>
<p>打开命令行窗口 输入<code>npm install hexo-cli -g</code>回车</p>
<p>在D盘根目录下创建MyBlog文件夹</p>
<p>打开命令行窗口在MyBlog目录下执行<code>hexo init</code>和<code>npm install</code>两个命令</p>
<p>继续在命令行中输入<code>hexo s -g</code>本地部署并启动服务</p>
<p>在浏览器地址栏中输入 <a href="http://localhost:4000/">http://localhost:4000</a> 即可看到博客主页</p>
<p>Github上创建名字为&lt;Github用户名&gt;.github.io的仓库</p>
<p>配置ssh秘钥</p>
<p>对MyBlog根目录下的配置文件_config.yml内容的结尾处作如下修改：</p>
<figure class="highlight dts"><table><tr><td class="code"><pre><span class="line"><span class="symbol">deploy:</span></span><br><span class="line"><span class="symbol">  type:</span> git</span><br><span class="line"><span class="symbol">  repository:</span> git@github.com:<span class="params">&lt;Github用户名&gt;</span>/<span class="params">&lt;Github用户名&gt;</span>.github.io.git</span><br><span class="line"><span class="symbol">  branch:</span> master</span><br></pre></td></tr></table></figure>

<p>在命令行中输入<code>npm install hexo-deployer-git --save</code>回车</p>
<p>在命令行中输入<code>hexo g -d</code>部署到github上，部署之前可以使用<code>hexo clean</code>清空缓存</p>
<p>在阿里云万网购买域名并添加解析设置</p>
<p>登录GitHub，进入之前创建的仓库，点击settings，设置Custom domain，输入&lt;购买的域名&gt;</p>
<p>在MyBlog的source目录下新建文件CNAME并修改其中内容为&lt;购买的域名&gt;</p>
<p>在命令行中输入<code>hexo g -d</code>部署到github上，部署之前可以使用<code>hexo clean</code>清空缓存</p>
<p>打开命令行窗口在MyBlog目录下执行hexo new &lt;文章名字&gt;</p>
<p>在MyBlog\source\_posts目录下会生成名称为&lt;文章名字&gt;.md的markdown文件</p>
<p>打开即可编辑，先通过<code>hexo s -g</code>本地测试再通过<code>hexo g -d</code>远程部署</p>
<p>每次执行命令部署命令前最好都要通过<code>hexo clean</code>清空缓存</p>
<p>美化博客：搜索next7相关美化教程</p>
<p>图床：github + jsdelivr<br>绘图：draw.io</p>
]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>博客</tag>
      </tags>
  </entry>
  <entry>
    <title>南航软件工程专硕考研经验贴</title>
    <url>/post/802f36c9.html</url>
    <content><![CDATA[<h3 id="2020年最新情况"><a href="#2020年最新情况" class="headerlink" title="2020年最新情况"></a>2020年最新情况</h3><p>2020考研专硕的<strong>计算机技术</strong>和<strong>软件工程</strong>两个专业合并成了一个专业，即<strong>电子信息</strong>，专业课和学硕考同一份卷子，增加了<strong>计算机组成原理</strong>的考查，今年专硕两个专业合并后继续呈现扩招趋势。</p>
<a id="more"></a>

<h3 id="我的考研初试情况"><a href="#我的考研初试情况" class="headerlink" title="我的考研初试情况"></a>我的考研初试情况</h3><table>
<thead>
<tr>
<th align="center">政治</th>
<th align="center">英语</th>
<th align="center">数学</th>
<th align="center">专业课922</th>
<th align="center">总分</th>
</tr>
</thead>
<tbody><tr>
<td align="center">70+</td>
<td align="center">65+</td>
<td align="center">120+</td>
<td align="center">135+</td>
<td align="center">390+</td>
</tr>
</tbody></table>
<h3 id="2019年软工专硕招生情况"><a href="#2019年软工专硕招生情况" class="headerlink" title="2019年软工专硕招生情况"></a>2019年软工专硕招生情况</h3><p>国家线于3月15日公布，学院复试名单及细则于3月22日晚上接近9点公布，复试是3月30日和31日两天，比大部分211学校晚一周，所以报考南航软专的同学必须耐下性子，背水一战，否则到后面调剂都来不及了。软专计划招35人，进复试75人，放弃9人，实际招了37人，比例接近1：2，相比去年略有扩招，复试竞争依旧激烈。</p>
<h3 id="公共课"><a href="#公共课" class="headerlink" title="公共课"></a>公共课</h3><p>初试科目中政治、英语、数学没什么好说的，跟好三个老师就行，按前面的科目顺序分别是：徐涛、朱伟、张宇，其他老师没跟过不好评价，这三位老师都有自己的微博和微信公众号，每天保持关注，出什么课就听什么课，出什么书就看什么书，都尽量去完成。</p>
<h3 id="专业课"><a href="#专业课" class="headerlink" title="专业课"></a>专业课</h3><p>专业课是<strong>数据结构</strong>和<strong>操作系统</strong>两门。<br>辅导书的话王道和天勤是首选，王道的我两本都买了，天勤只买了数据结构。<br>数据结构方面天勤讲得很详细，适合基础不是太好的同学，有需要可以买一下他们的课程，讲得通俗易懂，算法都是带着一步一步讲的，还有动画演示。题目推荐做王道的，王道上的编程题在考试中出现过类似题。如果你操作系统没学过或者和我一样学了等于没学，真心推荐王道的操作系统课程，听两遍绝对有好处，第一遍重理解，第二遍重记忆。我记得今年和去年分别有一道大题是书上没有视频里却补充到的，有心人一定能做出来。最后我想提醒一下选择题也不要忽视哦，虽然考试不大可能考，但是对于知识的理解还是帮助很大的，后面都有解析，做完可以校对一下。开始复习的时候可以按上面的来，但是后期一定要做真题，真题我认为只需做2012年往后的，真题至少做两遍，相似题目会反复考查，最后从真题涉及的知识点出发再回去翻辅导书查漏补缺。至于考纲中提到的两本参考书可以买来看看，不过个人觉得作用不大。</p>
<h3 id="复试"><a href="#复试" class="headerlink" title="复试"></a>复试</h3><h4 id="笔试"><a href="#笔试" class="headerlink" title="笔试"></a>笔试</h4><p>考查<strong>英语听力</strong>、<strong>离散数学</strong>、<strong>编译原理</strong>，本科这几样没什么印象的一定要早做准备，最好初试过后就要开始着手。英语听力分三个部分：短对话、篇章和谈话，没有了去年的长对话和短文判断正误，共30道选择题，难度明显加大，语速也很快，所以听力还是要好好练的，推荐星火英语的听力书。离散数学比较简单，编译原理题型固定，都是套路，但题目会很复杂，也会出现往年没考过的但属于基本题型的题目，还是需要多加练习。可以买一下“校研社”的复试资料，里面有近几年的复试真题和参考答案。我还是复试前一天才知道有这个资料的（我用的资料里面只有回忆版，题目并不全）。在此特别感谢那位临时认识的传给我pdf文档的那位同学，考前把近两年真题刷了一遍，考试的时候发现出题风格与往年基本一致。所以近几年的复试真题还是很重要的，至于这个资料在哪买大家可以去打听一下。</p>
<h4 id="面试"><a href="#面试" class="headerlink" title="面试"></a>面试</h4><p>分了4个组，4个组同时进行，我正好抽到第一个进去，开始有点紧张，一共6个老师。我是坐着的，两个老师坐在我正前方，两个坐在左前方一前一后，两个在右前方一前一后。先是英文自我介绍，接着回答一开始从信封中抽的三道题，三道题按题目难度分为低中高三个档次，第一题是问数据库中有哪些数据模型，我想了一会回答了关系模型，老师居然不确定，还去看了一下答案才告诉我关系模型是对的，但是还有其他模型让我想，后来想不出来就接着下一题了。第二题是问的硬件方面的，电子技术，组成原理，微机原理和系统结构这几门课之间的关系，第三题是有三种颜色变色龙各有多少多少只（具体几只忘了），其中两种颜色的变色龙相遇会变成另一种颜色，请问他们是否会变成同一种颜色。后面两题真的不懂就随便说了点，后来一个老师问了我项目，一个老师问了我对什么方向感兴趣，为什么要选择软件工程，还有一个问了我C语言内存泄漏的知识，还问我如何编一个程序去监测内存泄漏。我都不是回答得特别好，但是整个过程还算顺畅。估计最后我复试差都是因为面试分数太低的原因，一方面是第一个进去老师没有参考可能打分较低，另一方面也可能我确实表现没别人好，所以这方面也没什么值得参考的经验给大家。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>我初试成绩还行，但是复试直接翻了车，综合排名瞬间靠后，不过幸运的是没有被淘汰，不过我也感觉自己尽力了。大家准备初试的时候也可以适当争取些面试能拿得出手的和专业相关的经历，初试过后要继续努力，否则复试也很容易被刷掉。</p>
<hr>
<h3 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h3><h4 id="面试题"><a href="#面试题" class="headerlink" title="面试题"></a>面试题</h4><p>针对复试面试的一道题，刚刚在网上搜索了一下，发现了答案，记录在这里以备后人查看。<a href="https://www.nowcoder.com/questionTerminal/7f0379b9f8f44bd3a2bc51ed92642b23?orderByHotValue=1&page=1&onlyReference=false">原文链接</a></p>
<p>在一座荒岛上有三种变色龙，分别是12只棕色，15只灰色以及16只黑色。当两只不同颜色的变色龙相遇时他们会同时变色为另外一种颜色，例如当1只棕色和1只灰色的变色龙相遇时他们会同时变成黑色的。请问这个荒岛上这些变色龙可能全部变成同一种颜色吗？请详细说明你的分析过程。</p>
<p>问题换成13，15，16也是一样的思路，就是让其中两种颜色的各一只相遇则这两种颜色的个数各减少一只，相应的另一种颜色的增加两只，然后就会出现某两种颜色的数量一样，让这两种颜色的变色龙全部相遇，则全部变成另一种颜色。  数字都是设置好的，答案是<strong>能变一样颜色</strong>。</p>
<h4 id="简历"><a href="#简历" class="headerlink" title="简历"></a>简历</h4><p>想补充一下，面试的时候我带简历了，可是进去的时候一位老师直接说把资料都给他，然后我居然就忘记了给每位老师发简历。唉，自己辛苦准备的简历老师都没看到，我想也许这也是我面试低分的原因之一吧。所以大家在面试前一定要记住自己进去后第一件事是发简历，而不要受到一些不确定因素的干扰。不过我面试中途提到了我有简历，结果只是几个老师拿出来看了看，而不是所有人都看到。没有简历老师就不能针对性地提问，对你印象也不会太好，因为大家应该都是有简历的。</p>
]]></content>
      <categories>
        <category>考研</category>
      </categories>
      <tags>
        <tag>经验贴</tag>
      </tags>
  </entry>
  <entry>
    <title>怎么评价朱伟的恋练有词？</title>
    <url>/post/da7b8e17.html</url>
    <content><![CDATA[<h3 id="个人情况"><a href="#个人情况" class="headerlink" title="个人情况"></a>个人情况</h3><p>单词没有专门背过，全程跟着朱伟。从大三上学期开始看朱伟视频，新老版恋练有词各一遍，词组背多分一遍。最后英语一67，感觉自己主要是时间没控制好，导致后来翻译题最后两题来不及写，不然上70还是很有希望的。</p>
<a id="more"></a>

<h3 id="评价"><a href="#评价" class="headerlink" title="评价"></a>评价</h3><p>朱伟老师的恋练有词视频很多人会吐槽废话广告太多，其实那些所谓的“废话”确实在我考研的路上给了我很大的鼓舞和力量；那些广告其实很少，不想听直接跳过就行，不过由于我全程跟朱伟，那些广告对我还是挺有用的，这样我就知道下一阶段该买哪些书。</p>
<p>视频里绝不仅仅只有单词这么简单，涉及到的内容可以说包揽所有题型，对于语感的提升和英文实力的提高很有用，比自己去找东西背，找题目做效率高太多。不过听过的课要做笔记，课下要经常复盘。半个小时的课我往往要听一个多小时，除了暂停做笔记之外就是当场记住那些补充的东西。</p>
<p>考研期间有一个能在精神上不断支持自己的老师或同学很重要，所幸有了朱伟老师。我时常看他的微博和微信公众号，没有一次错过他的直播，哪怕是第一天考完结束后他的十分钟打气直播，我也准时看了。那时候我正好从南航自习室看完书回宾馆，一路上我就在听他的直播，周围车水马龙，我的内心一片宁静。直播内容基本就是谈谈心，说说他的近况，再打打气，感觉很幸福~~</p>
<p>考研结束后，我看了他在武汉举办的520高分盛典直播，他做了一场演讲——<a href="https://www.bilibili.com/video/av73348783">拥抱生活的不确定</a>。是啊，人生就是这样，充满了不确定性，在考研的过程中也不能确定自己一定能考上，但我始终选择相信自己。</p>
]]></content>
      <categories>
        <category>考研</category>
      </categories>
      <tags>
        <tag>知乎</tag>
      </tags>
  </entry>
  <entry>
    <title>时间序列研究资料汇总</title>
    <url>/post/5a198660.html</url>
    <content><![CDATA[<p>这篇文章整理了我在研究生期间所有看过的时间序列相关的研究资料，可以作为入门学习和深入研究时间序列预测或时间序列其他研究方向的参考。</p>
<a id="more"></a>

<h3 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h3><ul>
<li><a href="https://mp.weixin.qq.com/s/56so2p7a4wIgo38nVSR44A">【时间序列】时间序列分解总结</a></li>
<li><a href="https://blog.csdn.net/FrankieHello/article/details/88069870?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param">时间序列分析之熵（Entropy）_敲代码的quant的博客-CSDN博客</a></li>
<li><a href="https://machinelearningmastery.com/convert-time-series-supervised-learning-problem-python/">How to Convert a Time Series to a Supervised Learning Problem in Python</a></li>
<li><a href="https://blog.csdn.net/weixin_40683253/article/details/80878247">降维方法 -简直太全！- 附Python代码（Random Forest、Factor Analysis、corr、PCA、ICA、IOSMA）黄小包-CSDN博客</a></li>
<li><a href="https://blog.csdn.net/weixin_40532625/article/details/88797180?ops_request_misc=%7B%22request_id%22:%22160377237019725222401261%22,%22scm%22:%2220140713.130102334.pc_all.%22%7D&request_id=160377237019725222401261&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_v2~rank_v28-8-88797180.pc_first_rank_v2_rank_v28&utm_term=%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97+%E7%86%B5&spm=1018.2118.3001.4187">时间序列复杂性的度量—近似熵和样本熵_weixin_40532625的博客-CSDN博客</a></li>
<li><a href="https://blog.csdn.net/anaijiabao/article/details/106679271?ops_request_misc=%7B%22request_id%22:%22160377237019725222401261%22,%22scm%22:%2220140713.130102334.pc_all.%22%7D&request_id=160377237019725222401261&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_v2~rank_v28-5-106679271.pc_first_rank_v2_rank_v28&utm_term=%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97+%E7%86%B5&spm=1018.2118.3001.4187">用python做时间序列预测七：时间序列复杂度量化_程序员一一涤生-CSDN博客</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/306315024?utm_source=wechat_session&utm_medium=social&utm_oi=786305254336638976">时序数据的表征学习方法（二）——time2vec - 知乎</a></li>
<li><a href="https://blog.csdn.net/zhou85xin/article/details/84216393?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.control&dist_request_id=5f2a1e83-a951-471c-850f-fe3efd9e46e8&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.control">时间序列分析(一) 如何判断序列是否平稳_浪不虚传-CSDN博客</a></li>
<li><a href="https://mp.weixin.qq.com/s?__biz=Mzg3NDUwNTM3MA==&mid=2247484837&idx=2&sn=7a454c8b0368d8fb20a9c9c208b3d097&chksm=cecef050f9b97946babaf6c0109ac692502b0b785d11937df69fac837f5fe87cac4692c2b494&scene=21#wechat_redirect">【TS技术课堂】时间序列统计分析</a></li>
<li><a href="https://mp.weixin.qq.com/s?__biz=Mzg3NDUwNTM3MA==&mid=2247485154&idx=1&sn=e2b24a279902b6ddee1c3b83a9e3dd56&chksm=cecef317f9b97a01b86245fc7a8797b7618d752fdf6a589733bef3ddd07cb70a30f78b538899&scene=21#wechat_redirect">【TS技术课堂】时间序列特征工程</a></li>
</ul>
<h3 id="传统方法"><a href="#传统方法" class="headerlink" title="传统方法"></a>传统方法</h3><ul>
<li><a href="https://mp.weixin.qq.com/s/QZ_AcfzuB7JQEE6cDz5G1A">【时间序列】自回归模型</a></li>
<li><a href="https://blog.csdn.net/meng_shangjy/article/details/79714642?utm_medium=distribute.pc_relevant.none-task-blog-baidujs-1">时间序列分析之ARIMA上手-Python_meng_shangjy的博客-CSDN博客</a></li>
<li><a href="https://mp.weixin.qq.com/s/15HXAIhmtYLbG3MjwEKDSQ">【时间序列】从移动平均到指数平滑</a></li>
<li><a href="https://blog.csdn.net/foneone/article/details/90141213">时间序列模型（ARIMA和ARMA）完整步骤详述_越努力 越幸运-CSDN博客</a></li>
<li><a href="https://blog.csdn.net/qq_36558948/article/details/79594999?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-2.channel_param&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-2.channel_param">基于时间序列的短期数据预测–ARMA模型的设计与实现_tyxr-CSDN博客</a></li>
<li><a href="https://machinelearningmastery.com/arima-for-time-series-forecasting-with-python/">How to Create an ARIMA Model for Time Series Forecasting in Python</a></li>
<li><a href="https://www.kaggle.com/thebrownviking20/everything-you-can-do-with-a-time-series/notebook">Everything you can do with a time series | Kaggle</a></li>
<li><a href="https://mp.weixin.qq.com/s/fMkxWLGSKQm_3fyfLgbQew">【时间序列】详解Prophet模型以及代码示例</a></li>
<li><a href="https://mp.weixin.qq.com/s/YITnJCB0jZDf6gGe4e6j2w">【时间序列】Holt-Winters 指数平滑方法及其 Python 实践</a></li>
<li><a href="https://mp.weixin.qq.com/s/yfBJVwP1XddVakKsxRuv3g">【时间序列】时序分析之移动平均-python实战</a></li>
<li><a href="https://mp.weixin.qq.com/s/N3hCA1ipxZAJUeMTk7xZuQ">【时间序列】时间序列统计分析相关知识的总结与梳理</a></li>
<li><a href="https://mp.weixin.qq.com/s/QgivTfkAt5Kj54pRsdLBpQ">时间序列丨九种经典的基于「自回归+移动平均」序列预测模型及其 Python 实践</a></li>
<li><a href="https://mp.weixin.qq.com/s/PAxATtPfvayDEu1VYM5igA">深入浅出：隐马尔科夫模型</a></li>
<li><a href="https://mp.weixin.qq.com/s/4iB2klu5zHHq6mbT8nwYng">【时间序列】简单易用 &amp; 炒股必看的时序预测基本方法–移动平均（SMA、EMA、WMA）</a></li>
<li><a href="https://mp.weixin.qq.com/s/2PEEKGmpQAWyEw4bvfRahg">一次指数平滑、二次指数平滑、三次指数平滑（Holt-Winters）</a></li>
<li><a href="https://mp.weixin.qq.com/s/RDtaD0DPCGzJJfel1kMF_A">动手实战 | Statsmodels 中经典的时间序列预测方法</a></li>
<li><a href="https://mp.weixin.qq.com/s/8Q_THhsLRKBVSX3exSjXbQ">决策树、随机森林、bagging、boosting、Adaboost、GBDT、XGBoost总结</a></li>
<li><a href="https://mp.weixin.qq.com/s/fWWlPvnh39kvf-LbM8nHMA">硬核详细讲解 贝叶斯回归和高斯过程回归</a></li>
</ul>
<h3 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h3><ul>
<li><a href="https://zhuanlan.zhihu.com/p/44121378">【NLP】Transformer详解 - 知乎</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/105638031">使用PyTorch进行LSTM时间序列预测 - 知乎</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/257826109?utm_source=wechat_session&utm_medium=social&utm_oi=786305254336638976">告别RNN，迎接TCN - 知乎</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/161858544">时间序列的LSTM预测 - 知乎</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/63837980?utm_source=wechat_session&utm_medium=social&utm_oi=786305254336638976">基于EMD-SLSTM的家庭用电预测 - 知乎</a></li>
<li><a href="https://mp.weixin.qq.com/s/pZKrsGo55hlkL32Q9fqcDw">Transformer 超详细解读，一图胜千言</a></li>
<li><a href="https://mp.weixin.qq.com/s/bROPXK1ysv5m1x0BgZdfcA">Attention 扫盲：注意力机制及其 PyTorch 应用实现</a></li>
<li><a href="https://mp.weixin.qq.com/s/z2LoefaFlcex2XH5KMvI4A">【串讲总结】序列模型结构之普通堆叠、Encoding-Decoding、Encoding-Forcasting</a></li>
<li><a href="https://mp.weixin.qq.com/s/GGvlNA383y4vYpbkRzkhNA">【重温序列模型】再回首DeepLearning遇见了Transformer</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/188400946?utm_source=qq">时间卷积网络：时间序列的下一场革命？ - 知乎</a></li>
<li><a href="https://mp.weixin.qq.com/s/z4D-WCSVWjiPcH5kptWcbA">【重温序列模型】再回首DeepLearning遇见了LSTM和GRU</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/47929039">Tensorflow中的Seq2Seq全家桶 - 知乎</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/232650092">告别RNN，迎来TCN！股市预测任务是时候拥抱新技术了 - 知乎</a><a href="https://zhuanlan.zhihu.com/p/288400654">周期特征的循环编码 - 知乎</a></li>
<li><a href="https://mp.weixin.qq.com/s/KXX12kXQRLXthYoUOZScuA">【TS深度学习】时间卷积神经网络</a></li>
<li><a href="https://mp.weixin.qq.com/s/zNFEknTz0ZhCryhCMMCgRg">【图神经网络】GraphSAGE</a></li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzIyNjM2MzQyNg==&mid=2247553859&idx=1&sn=2352e56296bdf57320990a0dd4baa575&chksm=e873c20edf044b184692861cc80ca3605a72fc669d5b6e78bb58033ec60fec05d490ecac3669&scene=132#wechat_redirect">Transformers资料汇总！从原理到应用</a></li>
<li><a href="https://mp.weixin.qq.com/s/F18BUqIpDxBNat5vqZrwdw">熬了一晚上，我从零实现了Transformer模型，把代码讲给你听</a></li>
<li><a href="https://mp.weixin.qq.com/s/v5oUVqrUCUf9EtTO3eEazw">【时间序列】DeepAR: 自回归RNN预测时序概率分布</a></li>
</ul>
<h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><ul>
<li><a href="https://machinelearningmastery.com/time-series-datasets-for-machine-learning/">7 Time Series Datasets for Machine Learning</a></li>
<li><a href="https://www.kaggle.com/sohier/30-years-of-european-wind-generation?select=TS.CF.N2.30yr.csv">30 Years of European Wind Generation | Kaggle</a></li>
<li><a href="https://mp.weixin.qq.com/s/PLKbpYFjhayad-ppyFedtg">超全必看！开源时间序列数据集整理</a></li>
</ul>
<h3 id="其他方法"><a href="#其他方法" class="headerlink" title="其他方法"></a>其他方法</h3><ul>
<li><a href="https://blog.csdn.net/qq_37177765/article/details/102828997?utm_medium=distribute.pc_relevant_download.none-task-blog-baidujs-5.nonecase&depth_1-utm_source=distribute.pc_relevant_download.none-task-blog-baidujs-5.nonecase">混沌性时间序列的分析方法：EEMD+相空间重构_qq_37177765的博客-CSDN博客</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/32910931">时间序列模型之相空间重构模型 - 知乎</a></li>
<li><a href="https://blog.csdn.net/qq_30142403/article/details/80554919">时间序列模型之相空间重构_qq_30142403的博客-CSDN博客</a></li>
</ul>
<h3 id="文档"><a href="#文档" class="headerlink" title="文档"></a>文档</h3><ul>
<li><a href="https://www.tensorflow.org/tutorials/structured_data/time_series">Time series forecasting  | TensorFlow Core</a></li>
<li><a href="https://pyemd.readthedocs.io/en/latest/index.html">PyEMD’s documentation — PyEMD 0.2.9 documentation</a></li>
<li><a href="https://www.learnlatex.org/en/">Learn LaTeX online for free in beginner friendly lessons | learnlatex.org</a></li>
</ul>
<h3 id="博客"><a href="#博客" class="headerlink" title="博客"></a>博客</h3><ul>
<li><a href="http://yzhao.top/2018/10/27/20181026/">论文阅读综述（关于序列数据） | 洋仔的世界</a></li>
<li><a href="https://www.cnblogs.com/foley/p/5582358.html">python时间序列分析 - 大熊猫淘沙 - 博客园</a></li>
<li><a href="https://mp.weixin.qq.com/s/hbql2Fi5OSh1mt6eQAGDBQ">【时间序列】时间序列基本概念总结</a></li>
<li><a href="https://mp.weixin.qq.com/s/3jlKhxNbViqA32Bguj61ug">PyTorch 常用代码段合集</a></li>
<li><a href="https://www.linuxcool.com/">Linux命令大全(手册) – 真正好用的Linux命令在线查询网站</a></li>
<li><a href="https://mp.weixin.qq.com/s/T5YXPnHoA9pe_lKgQ2bWPw">【时间序列】python与时间序列基本教程4(超过1.9万字 代码超过900行 包括49个图)</a></li>
<li><a href="https://blog.csdn.net/longxinchen_ml/article/details/86533005">图解Transformer（完整版）_龙心尘-CSDN博客_transformer</a></li>
<li><a href="https://gitee.com/yuanzhoulvpi/time_series">hz/time_series - 码云 - 开源中国</a><a href="https://mp.weixin.qq.com/s/0fe1I-H9KopPtsaF1U-E3Q">时间序列多步预测的五种策略</a></li>
<li><a href="https://mp.weixin.qq.com/s/j7H7rngU3RZq_0TLzaCITA">【TS技术课堂】时间序列聚类</a></li>
<li><a href="https://mp.weixin.qq.com/s/0-P7Q2e1me9_HCX53SAWxA">NumPy 图解教程！</a></li>
<li><a href="https://mp.weixin.qq.com/s/xHEIqZRFs2WZeE49MoEBvQ">时间序列中如何进行交叉验证</a></li>
<li><a href="https://mp.weixin.qq.com/s?__biz=Mzg3NDUwNTM3MA==&mid=2247484974&idx=1&sn=d841c644fd9289ad5ec8c52a443463a5&chksm=cecef3dbf9b97acd8a9ededc069851afc00db422cb9be4d155cb2c2a9614b2ee2050dc7ab4d7&scene=21#wechat_redirect">【TS技术课堂】时间序列预测</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/136854657?utm_source=wechat_session&utm_medium=social&utm_oi=786305254336638976">论文画图神器！25个常用Matplotlib图的Python代码，收藏收藏！ - 知乎</a></li>
<li><a href="https://mp.weixin.qq.com/s/F1L-FIAJRmtuk9WWEQOPzA">做时间序列预测有必要用深度学习吗？事实证明，梯度提升回归树媲美甚至超越多个DNN模型</a></li>
</ul>
<h3 id="论文"><a href="#论文" class="headerlink" title="论文"></a>论文</h3><ul>
<li><a href="https://zhuanlan.zhihu.com/p/347237171?utm_source=wechat_session&utm_medium=social&utm_oi=786305254336638976">AAAI 2021 | 时间序列论文一览 - 知乎</a></li>
<li><a href="https://mp.weixin.qq.com/s?__biz=Mzg3NDUwNTM3MA==&mid=2247484901&idx=1&sn=5af38aa7381ed732d612a55042412150&chksm=cecef010f9b9790679981d5e9949ae1a28528e342f2dd91fed0cdce254801487e6db69e8340b&scene=21#wechat_redirect">KDD 2020 | 时间序列相关论文一览（附原文源码）</a></li>
<li><a href="https://mp.weixin.qq.com/s/umnWLjnLRGn-leEHU6yDyg">WSDM 2021 | 时间序列转化为动态图进行表示</a></li>
<li><a href="https://mp.weixin.qq.com/s/i3KpgAcqNfb0f4uH-59kug">【时间序列】KDD 2021丨时间序列相关研究论文汇总</a></li>
<li><a href="https://mp.weixin.qq.com/s/SRiO0IqvIh9u0BgYdiwtQg">【时间序列】ICML 2021丨时间序列相关研究汇总</a></li>
<li><a href="https://mp.weixin.qq.com/s/IbuRENQ3hYp__As7CN5H-Q">【时间序列】ICDE 2021丨时间序列相关研究论文汇总</a></li>
<li><a href="https://mp.weixin.qq.com/s/AJr4-cvwDjklpGNfKqbM9A">KDD 2020 | MTGNN: 基于图神经网络的多变量时间序列预测模型</a></li>
<li><a href="https://mp.weixin.qq.com/s/oa1SkSClNNd1Ni4qyVJrVg">WWW 2021 | 时间序列论文一览</a></li>
<li><a href="https://mp.weixin.qq.com/s/5FSAy2JhHawR6yTfkWufcw">SDM 2021丨AttnAR：基于注意力和自回归的多元时间序列预测模型</a></li>
<li><a href="https://mp.weixin.qq.com/s/3u6quUM6V1lcGKAnMVYaPw">NeurIPS 2021 | 时间序列相关论文一览（附原文源码）</a></li>
<li><a href="https://mp.weixin.qq.com/s/H0s8UZeFSa_QrDfkw6dxDg">AAAI 2021 | 高效长序列预测模型: Informer</a></li>
</ul>
<h3 id="视频"><a href="#视频" class="headerlink" title="视频"></a>视频</h3><ul>
<li><a href="https://space.bilibili.com/1567748478/video">跟李沐学AI的个人空间_哔哩哔哩_bilibili</a></li>
<li><a href="https://www.bilibili.com/video/BV1J441137V6?spm_id_from=333.999.0.0">李宏毅-Transformer_哔哩哔哩_bilibili</a></li>
<li><a href="https://www.bilibili.com/video/BV1Hz4y167fD?spm_id_from=333.999.0.0">【合集】《时间序列分析及应用 R语言》系列课程视频_哔哩哔哩_bilibili</a></li>
<li><a href="https://www.bilibili.com/video/BV1554y147M7?spm_id_from=333.999.0.0">【范文来啦 || SCI论文解析 】全是亮点 || 源于数据高于数据_哔哩哔哩_bilibili</a></li>
<li><a href="https://www.bilibili.com/video/BV1XT4y1u7ca?spm_id_from=333.999.0.0">【毕导】建议收藏！如何在毕业答辩上一展雄风，成为答辩之神！_哔哩哔哩_bilibili</a></li>
<li><a href="https://www.bilibili.com/video/BV1Sg4y1q79G?spm_id_from=333.999.0.0">【毕导】学渣的绝地求生！如何快速肝出一篇优秀的学术论文？_哔哩哔哩_bilibili</a></li>
</ul>
]]></content>
      <categories>
        <category>研究</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
</search>
